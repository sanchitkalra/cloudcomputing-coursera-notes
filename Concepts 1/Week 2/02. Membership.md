---
title: Membership
layout: default
parent: Week 2
grand_parent: Concepts 1
nav_order: 2
---
## Mean time to failure

Mean time to failure is the expected time period when one machine out of our cluster of machines will fail. It can be calculated as:
$t = p/n$ 
t = MTTF; p = rate of failure of one machine; n = number of machines

When n is very large, we can assume that failures are happening all the time.

## Group membership Service

At each process in our group of processes is a membership list which maintains a list of most or all processes in the system and have not yet failed. This list is then accessed by another application like the Gossip protocol. 

The challenge is to keep this membership list up to date as processes join the system, as they fail, and as they leave. Another challenge is that this communication about the state of processes has to happen over unreliable communication mediums which can delay or drop packets. 

These lists come in two flavours:
1. Strongly consistent membership: A list that is consistent across all processes at all times in the system. Ex, virtual synchrony. 
2. Almost complete membership: Gossip style algorithms rely on this. These lists are weakly consistent but very close to complete lists. 

The other aspects are:
1. Dissemination: A service that disseminates information about changes in process state to other processes in the system.
2. Failure detector: A service that detects failures in the system. 

The above three things (the list, the disseminator, the failure detector) are running on all processes in the system. Among these many processes the goal is when a process fails, at least one (may be more) process quickly finds out about the crash, and then uses the dissemination service to disseminate this information to other processes quickly. 

## Failure Detectors

There are two desirable properties of failure detectors:
1. Completeness: Each failure is detected eventually by at least one non faulty process. 
2. Accuracy: This is no false failure detection, no false positives. 

We cannot achieve these two goals fully over an unreliable communication medium that can loose packets. In real systems we go for 100% completeness but loose some accuracy in the process. This is important because it is important to discover all failed processes to keep our data consistent but it's okay if we have to ask a process mistakenly identified as failed to leave and rejoin. 

Two properties important for the performance of a failure detector are:
1. Speed: Time to first detection of failure should be as small as possible. 
2. Scale: The load and number of messages per process should be uniformly distributed with low network overhead. Also we'd like to avoid single points of failure and bottlenecks. 

An important thing to keep in mind while designing a failure detector is that multiple processes may fail simultaneously and not necessarily one after the another and the detector should detect them all. 
### Centralised Heartbeating

Consider a process Pi which is to be detected as failed. Pi sends periodic heartbeats which are nothing but messages with a sequence number to another process Pj which keeps track of when Pi sent the last heartbeat. If Pj does not receive a heartbeat from Pi within a specified timeout it marks the process as failed. Just like Pi all the processes except Pj send heartbeats to Pj and this mechanism is complete except for Pj. But if Pj itself fails there is no guarantee as to who detects failures now.
### Ring Heartbeating

In this variant, the nodes are organised in a ring. Each process sends heartbeats to at least one of it's neighbours. Any receiving node Pj if does not receive a heartbeat from Pi within a timeout marks Pi as failed. This also has issues and failure may go undetected, and we also have to deal with repairing the ring which has overhead. 
### All to All Heartbeating

In this variant, all nodes send heartbeats to all other nodes and any process can mark another process as failed if it doesn't receive a heartbeat within a specified timeout. This has very high load but the advantage is that it is complete for all members of the system. One issue that could occur is when one node Pj is slow to receive heartbeats than the others and marks all the other nodes as failed in the system.
### Gossip Style Failure Detection

In this variant, all processes keep a local heartbeat counter with respect to their own clocks (this is imp since clocks across processes may not be synchronised). The heartbeat counter has 3 elements:
1. The identifier for a process
2. The heartbeat counter
3. The local time

Periodically nodes send their membership tables to some nodes selected at random. The receiving nodes merge the incoming list into their own lists. If the received heartbeat counter for any process is higher than the one already present, the recipient node updates it's membership table with the new counter value and set's it current local time of update. If a node's last updated value is beyond a certain threshold ($T_{fail}$), it is marked as failed. After waiting for another $T_{cleanup}$, the entry is removed from the membership table. There exist two separate timeouts because if we attempt to delete the entires right away, the entires may actually never be deleted from the system. If we receive the entry again within the second threshold period, we don't pay attention to it and delete it later. 

Analysing what happens if $T_{gossip}$ is decreased. Generally a gossip takes $O(logn)$ time to propagate (through N nodes) provided enough bandwidth (bandwidth per node is $O(N)$) is available. If the available bandwidth per node is decreased to $O(1)$ the dissemination time goes up to $O(n*logn)$. Essentially if $T_{gossip}$ is decreased => more bandwidth is available and gossips are sent out much quicker => shorter timeout for failure detection. $T_{gossip}$ is a tradeoff between bandwidth and failure detection time.

If now we increase $T_{fail}$ and $T_{cleanup}$ the false positive rate decreases as nodes that were mistakenly detected as failed have some more time to send their heartbeats. 