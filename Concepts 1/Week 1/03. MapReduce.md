Map is a function that is applied to a list, where each the function is applied to each element of the list and a new list is generated.
Reduce is a function that is applied to a set/group of records in batches and it outputs a single record. 

Map processes individual records to generate intermediate key-value pairs. Each record processing is independent of the other and duplicate key value pairs may be generated. If the dataset is very large, we can shard the dataset among a lot of parallel Map tasks. Map as implemented by Apache Hadoop internally sorts the key value pairs by the key using quick-sort. 

Reduce now takes this key-value pair output of the Map stage and reduces or merges the key value pairs on a per key basis. Since the keys are being grouped we want all key value pairs with one given key to appear at one given Reduce task so that their values can be merged together. Each key is mapped to a reducer, called partitioning. One common way of partitioning is using hashing, and then modulo the number of reduce servers. This way each key is assigned to one Reduce task. This leads to a fairly even load balancing across our Reduce tasks. Reduce as implemented by Apache Hadoop internally sorts the key value pairs by the key using merge-sort. 

The MapReduce scheduler has to take care of the following things:
1. Parallelising the Map
2. Sharding/Splitting the data across multiple Map tasks.
3. Transfer data from Map to Reduce.
4. Parallelise and schedule the Reduce tasks. 
5. Implement storage for Map input, Map output/Reduce input and Reduce output.
6. Ensure that no Reduce tasks run before all Maps are finished. Technically the shuffle traffic, ie, the data flowing between the Map and the Reduce task can begin before all Map tasks have finished.

In the cloud we want to run the Map jobs near the source of our data to reduce overheads. Then we want to output the records with the same key to the same Reduce tasks. The input and outputs are stored in a distributed file system on the same sever that the Map and Reduce tasks are run. These distributed FS store multiple copies of the data and the Map can fetch data from the copy that is closest to it (typically the copy on the same machine). The Map output is in fact stored on local server on which the Map task is running, and the Reduce input is read from these local/remote disks. This is because the shuffle traffic data needs to be read extremely fast and isn't really needed as it is not relevant to the user but only needed as input for the Reduce phase. Finally the Reduce output is written back to the distributed FS.

### Yarn Scheduler
YARN - Yet another resource negotiator.
- Treats each server a collection of containers (some CPU + memory). 
- 3 main components
	- Global Resource Manager (RM): Scheduling.
	- Per server node manager (NM): Daemon responsible for server specific management of a particular server and monitors tasks for failure.
	- Per application (job) Application master (AM): Runs on one of the servers and is responsible for negotiating containers with RM and NM. Also communicates with NM to find out if any died and tasks running on that particular server need to be rescheduled.
- A highly simplified example:
	- Suppose there are two nodes (node A (job A), and node B (job B); suppose each node can run only one container) in the system with one RM. Each node is a running a job independently and node A's AM requests the RM that it needs another container. Now there are two possible scenarios with respect to containers that can run on node B. If node B is busy with it's own task, the RM tells that there are no available containers for node A's job. RM puts node A's request in a queue (usually a FIFO queue). Now say the task on node B has finished and RM now sees that node B can run this request. RM contacts the AM for node A and tells it to contact the NM for node B and ask to run it's job on node B. 

### Dealing with failure

- NM sends heartbeats to RM. If NM stops sending heartbeats, the RM knows that that particular node has failed and starts sending notifications to all affected AMs and it is now each AM's responsibility to reschedule their tasks.
- The NM also keeps track of each task running on the server. The NM can either restart the task or inform the AM/NM if the task fails.
- The AM also sends heartbeats to the RM. If the AM fails, the RM restarts the AM which then syncs up with the AM's running tasks.
- RM itself can fail and we can use secondary RMs if the primary RM fails and continue processing using checkpoints.

The resource requests/notification messages are bundled with the heartbeat messages to reduce the number of messages in the system.

A slow server (slow can be in terms of CPU, memory, network etc.) can slow down the entire MapReduce task as none of the Reduce jobs can run until all the Map tasks are complete. The solution is to keep track of progress of each task of a MapReduce job. Using this progress data, the AM does **Speculative Scheduling** which is basically making a copy of the slow tasks on other servers which are completing the tasks relatively faster and finally whichever server finishes the task first marks the task as done, and the other task will be killed. 
### Locality

The servers in a datacenter are arranged into racks that have very fast interconnect and racks are connected to one another using core switches. The distributed FS' typically store 3 copies of the data blocks, usually two on one rack and one on another. The scheduler attempts to schedule the tasks on the servers that contain the data. This way tasks can be scheduled and fetched quickly without much overhead. This may not always be possible if the containers on these machines are already full. Then the scheduler attempts to schedule the tasks on the same rack. This way the faster intra-rack interconnect is used without sending data over the relatively slower core-switch. If even this is not possible, the RM schedules the task wherever a container is available. 

These strategies do not apply well to Reduce tasks because data to a Reduce task can come from many different Map tasks and hence the best we can do is schedule the Reduce task on the same rack the Map tasks are running on. 